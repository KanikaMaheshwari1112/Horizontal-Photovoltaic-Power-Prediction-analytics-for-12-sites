# -*- coding: utf-8 -*-
"""Horizontal Photovoltaic Power Prediction for 12 Sites.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-OcwBP09SxVOpJVqZZmbX3ro9Ul5dYJs

Import relevant libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline

"""Data loading"""

df = pd.read_csv('Pasion et al dataset.csv')

"""Data exploration"""

df.shape

df.info()

df.head()

df.describe()

# Plot histogram of target variable
plt.figure(figsize=(12,5))
sns.histplot(df.PolyPwr);
plt.xlabel('PolyPwr (W)');

sns.pairplot(df)

df_corr = df[['Location', 'Time', 'Latitude', 'Longitude', 'Altitude',
              'YRMODAHRMI', 'Month', 'Hour', 'Season', 'Humidity', 'AmbientTemp',
              'Wind.Speed', 'Visibility', 'Pressure', 'Cloud.Ceiling', 'PolyPwr']].corr()

# Generate a mask for the upper triangle
mask = np.triu(np.ones_like(df_corr, dtype=bool))

f, ax = plt.subplots(figsize=(11, 9))
sns.heatmap(df_corr, mask=mask, cmap='jet', vmax=.3, center=0, annot=True, fmt='.2f',
            square=True, linewidths=.5, cbar_kws={"shrink": .8});
plt.title('Correlation analysis for all sites');

"""Summary of data Explorartion
1. The correlation between Ambient temperature, humidity and power improved when using site level data.
2. Also, some input features have their correlation sign with target change for individual sites compared with total sites data (e.g windspeed).
3. Hence, it may be necessary to build machine learning models per site to get the best out of the models. It might also be worth trying to encode the site-specific features like site location.
4. Altitude and pressure are perfectly correlated however, altitude does not change for a particular site and cannot be used for site-level modeling.

Data Preprocessing
"""

#checking for null values
df.isnull().sum()

"""No null values are present.

Feature Engineering

First, we perform the encoding of categorical variables namely location and season using the one-hot encoding method.
"""

# Encode location data
df_with_location_en = pd.get_dummies(df, columns=['Location'], drop_first=True)

# Encode season data
df_with_loc_season_en = pd.get_dummies(df_with_location_en, columns=['Season'], drop_first=True)

min_hour_of_interest = 10
max_hour_of_interest = 15

df_with_loc_season_en['delta_hr']= df_with_loc_season_en.Hour - min_hour_of_interest

"""It should be noted that only data between 10 am and 3 pm is available which cuts out the period when the systems are not expected to generate power."""

# Create Cyclic date features
df_with_loc_season_en['sine_mon']= np.sin((df_with_loc_season_en.Month - 1)*np.pi/11)
df_with_loc_season_en['cos_mon']= np.cos((df_with_loc_season_en.Month - 1)*np.pi/11)
df_with_loc_season_en['sine_hr']= np.sin((df_with_loc_season_en.delta_hr*np.pi/(max_hour_of_interest - min_hour_of_interest)))
df_with_loc_season_en['cos_hr']= np.cos((df_with_loc_season_en.delta_hr*np.pi/(max_hour_of_interest - min_hour_of_interest)))

mask2 = np.triu(np.ones_like(df_with_loc_season_en.corr(), dtype=bool))

f, ax = plt.subplots(figsize=(30, 20))
sns.heatmap(df_with_loc_season_en.corr(method='spearman'), mask=mask2, cmap='jet', vmax=.3, center=0, annot=True, fmt='.2f',
            square=True, linewidths=.5, cbar_kws={"shrink": .8});
plt.title('Correlation analysis including encoded features for all sites');

df_with_loc_season_en.head()

"""Additional correlation analysis including newly created features shows a perfect correlation between the cosine of date features and their actual values (Month and Hour). Hence, Month and Hour features are dropped in the modeling process.

Modeling

Feature Selection
"""

selected_columns = ['Latitude', 'Humidity', 'AmbientTemp', 'PolyPwr', 'Wind.Speed',
                     'Visibility', 'Pressure', 'Cloud.Ceiling', 'Location_Grissom',
                     'Location_Hill Weber', 'Location_JDMT', 'Location_Kahului',
                     'Location_MNANG', 'Location_Malmstrom', 'Location_March AFB',
                     'Location_Offutt', 'Location_Peterson', 'Location_Travis',
                     'Location_USAFA','Season_Spring', 'Season_Summer', 'Season_Winter',
                     'sine_mon', 'cos_mon', 'sine_hr', 'cos_hr']

df_processed = df_with_loc_season_en[selected_columns].reset_index(drop=True)

target_label = 'PolyPwr'
input_feat = list(set(selected_columns).difference(set([target_label])))

"""Summary of feature selection
1. Altitude is dropped because it has a high correlation with Pressure but does not change for a given location while presesure is more dynamic.
2. Longitude is dropped because it has zero correlation with the target variable.
3. Time, Hour, Month and Date are dropped because they have strong correlations with the engineered cyclic features but low correlation with the target variable.
"""

#Extract test set from all data
df_X = df_processed[input_feat].reset_index(drop=True)
df_y = df_processed[target_label]

X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size=0.2, random_state=42)



"""KNN Model """

# Scale training data
estimators = []
estimators.append(('standardize', StandardScaler()))
# estimators.append(('minmax', MinMaxScaler()))
estimators.append(('knn', KNeighborsRegressor(algorithm='ball_tree', weights='distance')))
knn_pipeline = Pipeline(estimators)

# Create list of each hyper-param to tune
n_neighbors_list = [5,10,15]
leaf_size_list = [20, 30, 40]
p_list = [2, 3, 4]

# Structure model hyper-params as a dictionary
knn_grid = {'knn__n_neighbors':n_neighbors_list,'knn__leaf_size':leaf_size_list,
             'knn__p':p_list}

# Create random search for  model
knn_random = RandomizedSearchCV(estimator=knn_pipeline, param_distributions=knn_grid, 
                                 n_iter=1000, cv=4, verbose=2, random_state=42, 
                                 n_jobs=-1)

knn_random.fit(X_train, y_train)

# Get optimal hyper-params
knn_random.best_params_

# Get score of best model during hyper-param tuning
knn_random.best_score_

mean_score = knn_random.cv_results_['mean_test_score']
std_score = knn_random.cv_results_['std_test_score']
params = knn_random.cv_results_['params']

cv_score_df = pd.DataFrame(params)
cv_score_df['mean_score'] = mean_score
cv_score_df['std_score'] = std_score

knn_model = knn_pipeline.set_params(**knn_random.best_params_)

knn_model.fit(X_train, y_train)

y_pred = knn_model.predict(X_test)

# Explained variance
explained_variance_score(y_test.ravel(), y_pred)

# R2 score
r2_score(y_test.ravel(), y_pred)

# Mean absolute error
mean_absolute_error(y_test.ravel(), y_pred)

# Root mean square error
np.sqrt(mean_squared_error(y_test.ravel(), y_pred))

mean_absolute_percentage_error(y_test.ravel(), y_pred)*100

"""RF model"""

# Create list of each hyper-param to tune
n_estimators_list = [300,900,1200,1800]

max_features_list = ['auto', 'sqrt']

max_depth_list = [5,10,15,20]
min_samples_split_list = [2, 5, 10]

min_samples_leaf_list = [1, 2, 4]

# Structure model hyper-params as a dictionary
rf_grid = {'n_estimators': n_estimators_list,
           'max_depth': max_depth_list,
           'min_samples_split': min_samples_split_list,
           'min_samples_leaf': min_samples_leaf_list}

from sklearn.ensemble import RandomForestRegressor

# Create base RF model
rf_base = RandomForestRegressor(random_state=42, max_features='sqrt',bootstrap='True')

# Create random search for RF model
rf_random = RandomizedSearchCV(estimator=rf_base, param_distributions=rf_grid, 
                                 n_iter=1000, cv=4, verbose=2, random_state=42, 
                                 n_jobs=-1)

# Fit the random search RF model
rf_random.fit(X_train, y_train)

# Get optimal hyper-params
rf_random.best_params_

# Get score of best model during hyper-param tuning
rf_random.best_score_

mean_score = rf_random.cv_results_['mean_test_score']
std_score = rf_random.cv_results_['std_test_score']
params = rf_random.cv_results_['params']

cv_score_df = pd.DataFrame(params)
cv_score_df['mean_score'] = mean_score
cv_score_df['std_score'] = std_score

rf_model = RandomForestRegressor(**rf_random.best_params_, random_state=42)

rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

# Explained variance
explained_variance_score(y_test.ravel(), y_pred)

# R2 score
r2_score(y_test.ravel(), y_pred)

# Mean absolute error
mean_absolute_error(y_test.ravel(), y_pred)

# Root mean square error
np.sqrt(mean_squared_error(y_test.ravel(), y_pred))

rf_model.feature_importances_

# Calculate scaled feature importance as a percentage
feat_imp_score = (rf_model.feature_importances_/max(rf_model.feature_importances_)*100)

feature_ranking_with_score = dict(sorted(zip(feat_imp_score, input_feat), reverse=True))
feature_ranking_with_score

feat_ranking_list = list(feature_ranking_with_score.values())
np.array(feat_ranking_list)

"""LGBM model"""

# Create list of each hyper-param to tune
n_estimators_list = [300, 600, 900, 1200]

learning_rate_list = [0.001, 0.005,0.01,0.1]

max_depth_list = [2,5,8,11]


n_leaves_list = [400,700,1000,1400]

feature_fraction_list = [0.1,0.2,0.4,0.6]

# Structure model hyper-params as a dictionary
lgbm_grid = {'num_leaves':n_leaves_list,
             'max_depth':max_depth_list,
             'learning_rate':learning_rate_list,
             'n_estimators':n_estimators_list,
             'feature_fraction':feature_fraction_list}

# Create base LGBM model
from lightgbm import LGBMRegressor
lgbm_base = LGBMRegressor(objective='rmse')

# Create random search for LGBM model
lgbm_random = RandomizedSearchCV(estimator=lgbm_base, param_distributions=lgbm_grid, 
                                 n_iter=1000, cv=4, verbose=2, random_state=42, 
                                 n_jobs=-1)

# Fit the random search LGBM model
lgbm_random.fit(X_train, y_train)

# Get optimal hyper-params
lgbm_random.best_params_

# Get score of best model during hyper-param tuning
lgbm_random.best_score_

mean_score = lgbm_random.cv_results_['mean_test_score']
std_score = lgbm_random.cv_results_['std_test_score']
params = lgbm_random.cv_results_['params']

cv_score_df = pd.DataFrame(params)
cv_score_df['mean_score'] = mean_score
cv_score_df['std_score'] = std_score

# Random
lgbm_model = LGBMRegressor(objective='rmse')

lgbm_model.fit(X_train, y_train)

y_pred = lgbm_model.predict(X_test)

# Explained variance
explained_variance_score(y_test.ravel(), y_pred)

# R2 score
r2_score(y_test.ravel(), y_pred)

# Mean absolute error
mean_absolute_error(y_test.ravel(), y_pred)

# Root mean square error
np.sqrt(mean_squared_error(y_test.ravel(), y_pred))

y_all_pred = lgbm_model.predict(df_X.values)

r2_score(df_y.values, y_all_pred)

# Calculate scaled feature importance as a percentage
feat_imp_score = (lgbm_model.feature_importances_/max(lgbm_model.feature_importances_)*100)

feature_ranking_with_score = dict(sorted(zip(feat_imp_score, input_feat), reverse=True))
feature_ranking_with_score

feat_ranking_list = list(feature_ranking_with_score.values())
np.array(feat_ranking_list)

# Define the base models
base0 = list()
base0.append(('lgbm', lgbm_model))
base0.append(('rf', rf_model))
base0.append(('knn', knn_model))

# Define meta learner model
from sklearn.linear_model import LinearRegression
base1 = LinearRegression()

# Define the stacking ensemble
from sklearn.ensemble import  StackingRegressor
stacked_model = StackingRegressor(estimators=base0, final_estimator=base1, cv=4, passthrough=True)

# Fit the model on the training data
stacked_model.fit(X_train, y_train)

y_pred = stacked_model.predict(X_test)

# Explained variance
explained_variance_score(y_test.ravel(), y_pred)

# R2 score
r2_score(y_test.ravel(), y_pred)

# Mean absolute error
mean_absolute_error(y_test.ravel(), y_pred)

# Root mean square error
np.sqrt(mean_squared_error(y_test.ravel(), y_pred))

"""Model             R^2
KNN               0.618
RF                0.667
LGBM              0.661

Results show that the best RF model has the highest R^2 score across all algorithms investigated.

Stacked model R^2 has the best performance.

Feature importance

LGBM model

1.   100.0: 'AmbientTemp
2.   93.39805825242719: 'Humidity'
3.   81.74757281553397: 'Pressure'
4.   56.699029126213595: 'Cloud.Ceiling'
5.   40.3883495145631: 'cos_mon'

 RF model
 

1.   100.0: 'AmbientTemp'
2.    22.51329665072026: 'Humidity',
3.    21.989004043280044: 'Cloud.Ceiling',
4.   19.723740315794824: 'Pressure',
5.   12.447282854818683: 'sine_mon'

Ambient temperature, humidity, cloud ceiling, and pressure are present in the top 5 features for both LGBM and RF models.
"""